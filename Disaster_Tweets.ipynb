{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1                   Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4                              Forest fire near La Ronge Sask. Canada   \n",
       "2   5                   All residents asked to 'shelter in place' are ...   \n",
       "3   6                   13,000 people receive #wildfires evacuation or...   \n",
       "4   7                   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('%20', ' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('@', ''))\n",
    "train, val = train_test_split(df, stratify=df['target'])\n",
    "\n",
    "test = pd.read_csv('data/test.csv')\n",
    "test = test.fillna('')\n",
    "test['text'] = test['text'].apply(lambda x: x.replace('%20', ' '))\n",
    "test['text'] = test['text'].apply(lambda x: x.replace('@', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "aftershock           1.000000\n",
       "derailment           1.000000\n",
       "debris               1.000000\n",
       "wreckage             1.000000\n",
       "body%20bags          0.951220\n",
       "outbreak             0.950000\n",
       "typhoon              0.947368\n",
       "oil%20spill          0.947368\n",
       "ruin                 0.945946\n",
       "blazing              0.941176\n",
       "suicide%20bombing    0.939394\n",
       "body%20bag           0.939394\n",
       "electrocute          0.937500\n",
       "suicide%20bomber     0.935484\n",
       "screaming            0.888889\n",
       "traumatised          0.885714\n",
       "panicking            0.878788\n",
       "blew%20up            0.878788\n",
       "blight               0.875000\n",
       "bombing              0.862069\n",
       "Name: pct_diff, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = pd.crosstab(df['keyword'], df['target'], normalize='index')\n",
    "cs['pct_diff'] = abs(cs[0].values - cs[1].values)\n",
    "\n",
    "cs['pct_diff'].sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy NLP, Tokenize, and Vectorize Text Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_doc(text):\n",
    "    doc = nlp(text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    return [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(doc):\n",
    "    return doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palme\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\palme\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train['docs'] = train['text'].apply(text_to_doc)\n",
    "val['docs'] = val['text'].apply(text_to_doc)\n",
    "test['docs'] = test['text'].apply(text_to_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palme\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\palme\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train['tokens'] = train['docs'].apply(tokenize)\n",
    "val['tokens'] = val['docs'].apply(tokenize)\n",
    "test['tokens'] = test['docs'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palme\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\palme\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train['vectors'] = train['docs'].apply(vectorize)\n",
    "val['vectors'] = val['docs'].apply(vectorize)\n",
    "test['vectors'] = test['docs'].apply(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5323    [hashtagteaclub, place, ex, Pars, defender, An...\n",
       "466     [volleyball, Attack, II, Volleyball, Training,...\n",
       "2966    [Drowning, Girl, Caitlin, R., Kiernan, Centipe...\n",
       "6658    [ALIPAPER, woman, get, problem, keepingthevigi...\n",
       "2565    [let, bring, matter, hard, try, beconfident, l...\n",
       "6243    [ÛÏLordBrathwaite, ahh, hate, snow, \\n\\n, lol...\n",
       "335     [samihonkonen, time, 23, hour, late, series, W...\n",
       "2405    [jozerphine, literally, look, yeah, derail, SM...\n",
       "5313    [family, sue, Legionnaires, 40, family, affect...\n",
       "3798    [toddler, Bedding, Firetruck, Bundle, Fire, Tr...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokens'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,3), max_df=.9, min_df=2, max_features=1000)\n",
    "clf = RandomForestClassifier(n_estimators=50, max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=20, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vect.fit_transform(train['text'])\n",
    "X_val = vect.transform(val['text'])\n",
    "X_test = vect.transform(val['text'])\n",
    "clf.fit(X_train, train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8043440182168505"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7484243697478992"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model\n",
    "clf.score(X_val, val['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5709, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [doc.vector for doc in train['docs']]\n",
    "X_val = [doc.vector for doc in val['docs']]\n",
    "X_test = [doc.vector for doc in test['docs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=20, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, train['target'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9891399544578735\n",
      "0.8046218487394958\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_train, train['target']))\n",
    "print(clf.score(X_val, val['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "params = {'n_estimators':range(50, 150),\n",
    "         'max_depth':range(10,50)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=None,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='deprecated', n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'max_depth': range(10, 50),\n",
       "                                        'n_estimators': range(50, 150)},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = RandomizedSearchCV(clf, params, verbose=1, n_jobs=-1, n_iter=50)\n",
    "grid.fit(X_train, train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7896305664831446"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 133, 'max_depth': 41}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9891399544578735\n",
      "0.8135504201680672\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_train, train['target']))\n",
    "print(grid.score(X_val, val['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "pred_sub = pd.DataFrame({'id':test['id'], 'target':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_sub = pd.read_csv('data/sample_submission.csv')\n",
    "samp_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sub.to_csv('word_embeddings_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "early_stop = EarlyStopping(patience=7)\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=300))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "179/179 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6236 - val_loss: 0.5929 - val_accuracy: 0.7526\n",
      "Epoch 2/200\n",
      "179/179 [==============================] - 0s 3ms/step - loss: 0.5797 - accuracy: 0.7224 - val_loss: 0.5151 - val_accuracy: 0.7768\n",
      "Epoch 3/200\n",
      "179/179 [==============================] - 0s 3ms/step - loss: 0.5461 - accuracy: 0.7471 - val_loss: 0.4854 - val_accuracy: 0.7883\n",
      "Epoch 4/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7527 - val_loss: 0.4691 - val_accuracy: 0.7973\n",
      "Epoch 5/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7600 - val_loss: 0.4605 - val_accuracy: 0.8072\n",
      "Epoch 6/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7702 - val_loss: 0.4504 - val_accuracy: 0.8062\n",
      "Epoch 7/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7718 - val_loss: 0.4452 - val_accuracy: 0.8125\n",
      "Epoch 8/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.7754 - val_loss: 0.4383 - val_accuracy: 0.8114\n",
      "Epoch 9/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.7761 - val_loss: 0.4361 - val_accuracy: 0.8167\n",
      "Epoch 10/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7795 - val_loss: 0.4318 - val_accuracy: 0.8193\n",
      "Epoch 11/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7874 - val_loss: 0.4283 - val_accuracy: 0.8188\n",
      "Epoch 12/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.7851 - val_loss: 0.4288 - val_accuracy: 0.8188\n",
      "Epoch 13/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7879 - val_loss: 0.4260 - val_accuracy: 0.8225\n",
      "Epoch 14/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7921 - val_loss: 0.4223 - val_accuracy: 0.8209\n",
      "Epoch 15/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.7942 - val_loss: 0.4204 - val_accuracy: 0.8167\n",
      "Epoch 16/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.8000 - val_loss: 0.4178 - val_accuracy: 0.8172\n",
      "Epoch 17/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7979 - val_loss: 0.4174 - val_accuracy: 0.8178\n",
      "Epoch 18/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4582 - accuracy: 0.7956 - val_loss: 0.4164 - val_accuracy: 0.8178\n",
      "Epoch 19/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.7954 - val_loss: 0.4162 - val_accuracy: 0.8214\n",
      "Epoch 20/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4480 - accuracy: 0.8007 - val_loss: 0.4137 - val_accuracy: 0.8214\n",
      "Epoch 21/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8005 - val_loss: 0.4124 - val_accuracy: 0.8199\n",
      "Epoch 22/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.7994 - val_loss: 0.4132 - val_accuracy: 0.8183\n",
      "Epoch 23/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8036 - val_loss: 0.4127 - val_accuracy: 0.8188\n",
      "Epoch 24/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8092 - val_loss: 0.4095 - val_accuracy: 0.8199\n",
      "Epoch 25/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4457 - accuracy: 0.8070 - val_loss: 0.4094 - val_accuracy: 0.8225\n",
      "Epoch 26/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8101 - val_loss: 0.4090 - val_accuracy: 0.8214\n",
      "Epoch 27/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.8056 - val_loss: 0.4094 - val_accuracy: 0.8241\n",
      "Epoch 28/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.8098 - val_loss: 0.4102 - val_accuracy: 0.8209\n",
      "Epoch 29/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.8066 - val_loss: 0.4094 - val_accuracy: 0.8204\n",
      "Epoch 30/200\n",
      "179/179 [==============================] - 1s 3ms/step - loss: 0.4300 - accuracy: 0.8157 - val_loss: 0.4078 - val_accuracy: 0.8214\n",
      "Epoch 31/200\n",
      "179/179 [==============================] - 0s 3ms/step - loss: 0.4269 - accuracy: 0.8170 - val_loss: 0.4061 - val_accuracy: 0.8235\n",
      "Epoch 32/200\n",
      "179/179 [==============================] - 1s 3ms/step - loss: 0.4331 - accuracy: 0.8045 - val_loss: 0.4082 - val_accuracy: 0.8209\n",
      "Epoch 33/200\n",
      "179/179 [==============================] - 0s 3ms/step - loss: 0.4236 - accuracy: 0.8131 - val_loss: 0.4071 - val_accuracy: 0.8246\n",
      "Epoch 34/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8133 - val_loss: 0.4056 - val_accuracy: 0.8204\n",
      "Epoch 35/200\n",
      "179/179 [==============================] - 1s 3ms/step - loss: 0.4282 - accuracy: 0.8085 - val_loss: 0.4068 - val_accuracy: 0.8209\n",
      "Epoch 36/200\n",
      "179/179 [==============================] - 0s 3ms/step - loss: 0.4277 - accuracy: 0.8133 - val_loss: 0.4069 - val_accuracy: 0.8204\n",
      "Epoch 37/200\n",
      "179/179 [==============================] - 1s 3ms/step - loss: 0.4227 - accuracy: 0.8149 - val_loss: 0.4054 - val_accuracy: 0.8209\n",
      "Epoch 38/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8173 - val_loss: 0.4045 - val_accuracy: 0.8209\n",
      "Epoch 39/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.8154 - val_loss: 0.4056 - val_accuracy: 0.8204\n",
      "Epoch 40/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.8140 - val_loss: 0.4060 - val_accuracy: 0.8183\n",
      "Epoch 41/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4175 - accuracy: 0.8185 - val_loss: 0.4046 - val_accuracy: 0.8230\n",
      "Epoch 42/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8213 - val_loss: 0.4043 - val_accuracy: 0.8225\n",
      "Epoch 43/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8236 - val_loss: 0.4054 - val_accuracy: 0.8188\n",
      "Epoch 44/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.8233 - val_loss: 0.4044 - val_accuracy: 0.8225\n",
      "Epoch 45/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8222 - val_loss: 0.4043 - val_accuracy: 0.8162\n",
      "Epoch 46/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8257 - val_loss: 0.4049 - val_accuracy: 0.8204\n",
      "Epoch 47/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8241 - val_loss: 0.4034 - val_accuracy: 0.8136\n",
      "Epoch 48/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8229 - val_loss: 0.4044 - val_accuracy: 0.8136\n",
      "Epoch 49/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8233 - val_loss: 0.4048 - val_accuracy: 0.8246\n",
      "Epoch 50/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4077 - accuracy: 0.8255 - val_loss: 0.4059 - val_accuracy: 0.8241\n",
      "Epoch 51/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8269 - val_loss: 0.4038 - val_accuracy: 0.8204\n",
      "Epoch 52/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8269 - val_loss: 0.4041 - val_accuracy: 0.8220\n",
      "Epoch 53/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8285 - val_loss: 0.4036 - val_accuracy: 0.8188\n",
      "Epoch 54/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8317 - val_loss: 0.4031 - val_accuracy: 0.8225\n",
      "Epoch 55/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8266 - val_loss: 0.4034 - val_accuracy: 0.8199\n",
      "Epoch 56/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8273 - val_loss: 0.4039 - val_accuracy: 0.8230\n",
      "Epoch 57/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8294 - val_loss: 0.4035 - val_accuracy: 0.8199\n",
      "Epoch 58/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.8348 - val_loss: 0.4035 - val_accuracy: 0.8183\n",
      "Epoch 59/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8273 - val_loss: 0.4037 - val_accuracy: 0.8204\n",
      "Epoch 60/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8303 - val_loss: 0.4032 - val_accuracy: 0.8199\n",
      "Epoch 61/200\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8276 - val_loss: 0.4034 - val_accuracy: 0.8193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2135446fe48>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, train['target'], epochs=200, validation_data=(X_val, val['target']), callbacks=[reduce_lr, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape((3263,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sub = pd.DataFrame({'id':test['id'], 'target':y_pred})\n",
    "pred_sub['target'] = pred_sub['target'].apply(lambda x: int(x))\n",
    "pred_sub.to_csv('simple_nn_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Choice('initial_units', [16, 32, 64, 128], default=16), activation='relu', input_dim=300))\n",
    "    if hp.Boolean('add_dropout'):\n",
    "        model.add(Dropout(hp.Float('dropout', .01, .8)))\n",
    "    for i in range(1, hp.Int('num_layers', 1, 4)):\n",
    "        model.add(Dense(hp.Choice(f'units_{i}', [16, 32, 64, 128]), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', .001, .1)), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTuner(RandomSearch):\n",
    "  def run_trial(self, trial, *args, **kwargs):\n",
    "    # You can add additional HyperParameters for preprocessing and custom training loops\n",
    "    # via overriding `run_trial`\n",
    "    kwargs['batch_size'] = trial.hyperparameters.Choice('batch_size', [32,64,128])\n",
    "#     kwargs['epochs'] = trial.hyperparameters.Int('epochs', 30, 70)\n",
    "    super(MyTuner, self).run_trial(trial, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = MyTuner(build_model, objective='val_accuracy', max_trials=200, executions_per_trial=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 200 Complete [00h 00m 07s]\n",
      "val_accuracy: 0.8224789897600809\n",
      "\n",
      "Best val_accuracy So Far: 0.8310574293136597\n",
      "Total elapsed time: 00h 41m 28s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, train['target'], epochs=100, validation_data=(X_val, val['target']), callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'initial_units': 128,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 4,\n",
       "  'learning_rate': 0.0016811370972802673,\n",
       "  'batch_size': 64,\n",
       "  'units_1': 128,\n",
       "  'dropout': 0.3105276090115756,\n",
       "  'units_2': 32,\n",
       "  'units_3': 128},\n",
       " {'initial_units': 16,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 4,\n",
       "  'learning_rate': 0.0053621070764740845,\n",
       "  'batch_size': 128,\n",
       "  'units_1': 32,\n",
       "  'dropout': 0.3832748803463333,\n",
       "  'units_2': 16,\n",
       "  'units_3': 128},\n",
       " {'initial_units': 16,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 2,\n",
       "  'learning_rate': 0.008893345372387871,\n",
       "  'batch_size': 128,\n",
       "  'units_1': 128,\n",
       "  'dropout': 0.37599956645412413,\n",
       "  'units_2': 32,\n",
       "  'units_3': 32},\n",
       " {'initial_units': 128,\n",
       "  'add_dropout': False,\n",
       "  'num_layers': 2,\n",
       "  'learning_rate': 0.027094340070893928,\n",
       "  'batch_size': 128,\n",
       "  'units_1': 16,\n",
       "  'dropout': 0.5365498664367073,\n",
       "  'units_2': 16,\n",
       "  'units_3': 32},\n",
       " {'initial_units': 32,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 1,\n",
       "  'learning_rate': 0.00578589138656092,\n",
       "  'batch_size': 128,\n",
       "  'units_1': 64,\n",
       "  'dropout': 0.2137477109258642,\n",
       "  'units_2': 64,\n",
       "  'units_3': 16},\n",
       " {'initial_units': 64,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 3,\n",
       "  'learning_rate': 0.04629436831402177,\n",
       "  'batch_size': 64,\n",
       "  'units_1': 16,\n",
       "  'dropout': 0.6038489070331148,\n",
       "  'units_2': 16,\n",
       "  'units_3': 32},\n",
       " {'initial_units': 32,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 3,\n",
       "  'learning_rate': 0.011568033707930019,\n",
       "  'batch_size': 32,\n",
       "  'units_1': 128,\n",
       "  'dropout': 0.3931224102755888,\n",
       "  'units_2': 128,\n",
       "  'units_3': 64},\n",
       " {'initial_units': 128,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 2,\n",
       "  'learning_rate': 0.012790559578257858,\n",
       "  'batch_size': 64,\n",
       "  'units_1': 64,\n",
       "  'dropout': 0.33504121308555446,\n",
       "  'units_2': 16,\n",
       "  'units_3': 128},\n",
       " {'initial_units': 64,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 2,\n",
       "  'learning_rate': 0.015833994987694198,\n",
       "  'batch_size': 64,\n",
       "  'units_1': 128,\n",
       "  'dropout': 0.3952043680211444,\n",
       "  'units_2': 32,\n",
       "  'units_3': 64},\n",
       " {'initial_units': 64,\n",
       "  'add_dropout': True,\n",
       "  'num_layers': 2,\n",
       "  'learning_rate': 0.026211565059175566,\n",
       "  'batch_size': 128,\n",
       "  'units_1': 128,\n",
       "  'dropout': 0.4359098982208583,\n",
       "  'units_2': 128,\n",
       "  'units_3': 64}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(10)\n",
    "[x.values for x in best_hp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.round(0)\n",
    "y_pred = y_pred.reshape((3263,))\n",
    "pred_sub = pd.DataFrame({'id':test['id'], 'target':y_pred})\n",
    "pred_sub['target'] = pred_sub['target'].apply(lambda x: int(x))\n",
    "pred_sub.to_csv('hp_tuned_nn_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Embedding Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=1000,oov_token='<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train['tokens'])\n",
    "X_val = tokenizer.texts_to_sequences(val['tokens'])\n",
    "X_test = tokenizer.texts_to_sequences(test['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', truncating='post')\n",
    "X_val = pad_sequences(X_val, len(X_train[0]), padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, len(X_train[0]), padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Dropout(.2))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "179/179 [==============================] - 7s 26ms/step - loss: 0.5620 - accuracy: 0.7073 - val_loss: 0.4745 - val_accuracy: 0.7878\n",
      "Epoch 2/200\n",
      "179/179 [==============================] - 6s 31ms/step - loss: 0.4425 - accuracy: 0.8012 - val_loss: 0.4609 - val_accuracy: 0.7973\n",
      "Epoch 3/200\n",
      "179/179 [==============================] - 5s 26ms/step - loss: 0.4205 - accuracy: 0.8163 - val_loss: 0.4391 - val_accuracy: 0.8046\n",
      "Epoch 4/200\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4089 - accuracy: 0.8238 - val_loss: 0.4516 - val_accuracy: 0.7815\n",
      "Epoch 5/200\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3972 - accuracy: 0.8324 - val_loss: 0.4442 - val_accuracy: 0.8046\n",
      "Epoch 6/200\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3908 - accuracy: 0.8341 - val_loss: 0.4332 - val_accuracy: 0.7978\n",
      "Epoch 7/200\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3875 - accuracy: 0.8366 - val_loss: 0.4512 - val_accuracy: 0.7983\n",
      "Epoch 8/200\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3813 - accuracy: 0.8364 - val_loss: 0.4712 - val_accuracy: 0.7952\n",
      "Epoch 9/200\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3782 - accuracy: 0.8392 - val_loss: 0.4865 - val_accuracy: 0.7820\n",
      "Epoch 10/200\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3764 - accuracy: 0.8434 - val_loss: 0.4416 - val_accuracy: 0.7978\n",
      "Epoch 11/200\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3714 - accuracy: 0.8462 - val_loss: 0.4487 - val_accuracy: 0.7957\n",
      "Epoch 12/200\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3573 - accuracy: 0.8509 - val_loss: 0.4586 - val_accuracy: 0.7962\n",
      "Epoch 13/200\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3554 - accuracy: 0.8523 - val_loss: 0.4823 - val_accuracy: 0.7952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213510d5978>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, train['target'], epochs=200, validation_data=(X_val, val['target']), callbacks=[EarlyStopping('val_accuracy', patience=10), reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.round(0)\n",
    "y_pred = y_pred.reshape((3263,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sub = pd.DataFrame({'id':test['id'], 'target':y_pred})\n",
    "pred_sub['target'] = pred_sub['target'].apply(lambda x: int(x))\n",
    "pred_sub.to_csv('embedding_nn_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
